{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# numpy实现梯度下降法\n",
    "以下为分别采用BGD、SGD、MBGD拟合y=3x_1+4x_2的系数[3, 4]的代码\n",
    "https://blog.csdn.net/xiaoxy97/article/details/83014243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 样本数为100条，特征数为二维\n",
    "def get_data(sample_num=100):\n",
    "    x1 = np.linspace(0, 9, sample_num)\n",
    "    x2 = np.linspace(4, 13, sample_num)\n",
    "    x = np.concatenate(([x1], [x2]), axis=0).T\n",
    "    y = np.dot(x, np.array([3, 4]).T) \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100,)\n"
     ]
    }
   ],
   "source": [
    "x,y = get_data()\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# BGD 批量梯度下降法(Batch Gradient Descent, BGD)，使用所有样本在当前点的梯度值来对变量参数进行更新操作\n",
    "def bgd(x, y, step_size=0.01, max_iter_count=10000):\n",
    "    w = np.ones((x.shape[1],)) #x.shape[1]为2，x的分量的个数\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "    loss = 10\n",
    "    iter_count = 0\n",
    "    while abs(loss) > 0.0001 and iter_count < max_iter_count:\n",
    "        w[0] -= step_size * (np.sum((w[0] * x1 + w[1] * x2 - y) * x1) / x.shape[0]) #步长*梯度，梯度是所有梯度的均值\n",
    "        w[1] -= step_size * (np.sum((w[0] * x1 + w[1] * x2 - y) * x2) / x.shape[0])\n",
    "        loss = np.sum(w[0] * x1 + w[1] * x2 - y)/x.shape[0] #loss不/x.shape[0] 求均值？\n",
    "        iter_count += 1\n",
    "        print(\"iter_count:%d    the loss:%f\" % (iter_count, loss))\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SGD 随机梯度下降法(Stochastic Gradient Descent, SGD),在更新变量参数的时候，随机选取一个样本的梯度值来更新参数。\n",
    "def sgd(x, y, step_size=0.01, max_iter_count=10000):\n",
    "    w = np.ones((x.shape[1],))\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "    loss = 10\n",
    "    iter_count = 0\n",
    "    while abs(loss) > 0.00001 and iter_count < max_iter_count:\n",
    "        i = np.random.randint(x.shape[0]) #sample_num个样本里随机选一个\n",
    "        w[0] -= step_size * (w[0] * x1[i] + w[1] * x2[i] - y[i]) * x1[i]\n",
    "        w[1] -= step_size * (w[0] * x1[i] + w[1] * x2[i] - y[i]) * x2[i]\n",
    "        loss = np.sum(w[0] * x1 + w[1] * x2 - y)\n",
    "        iter_count += 1\n",
    "        print(\"iter_count:%d    the loss:%f\" % (iter_count, loss))\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnumpy.random.randint(low, high=None, size=None, dtype='l')\\n函数的作用是，返回一个随机整型数，范围从低（包括）到高（不包括），即[low, high)。\\n如果没有写参数high的值，则返回[0,low)的值。\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSGD 小批量梯度下降法(Mini-batch Gradient Descent, MBGD),集合BGD和SGD的特性，从原始数据中，每次选择n个样本来更新参数值。\n",
    "   \n",
    "def msgd(x, y, batch_size, step_size=0.01, max_iter_count=10000): #每次batch_size个样本更新参数\n",
    "    w = np.ones((x.shape[1],))\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "    loss = 10\n",
    "    iter_count = 0\n",
    "    while abs(loss) > 0.00001 and iter_count < max_iter_count:\n",
    "        i = np.random.randint(x.shape[0], size=batch_size) #从sample_num个样本里随机选batch_size个\n",
    "        w[0] -= step_size * (np.sum((w[0] * x1[i] + w[1] * x2[i] - y[i]) * x1[i]) / batch_size)\n",
    "        w[1] -= step_size * (np.sum((w[0] * x1[i] + w[1] * x2[i] - y[i]) * x2[i]) / batch_size) #x1[i]这样操作可以\n",
    "        loss = np.sum(w[0] * x1 + w[1] * x2 - y)/ batch_size\n",
    "        iter_count += 1\n",
    "        print(\"iter_count:%d    the loss:%f\" % (iter_count, loss))\n",
    "    return w\n",
    "'''\n",
    "numpy.random.randint(low, high=None, size=None, dtype='l')\n",
    "函数的作用是，返回一个随机整型数，范围从低（包括）到高（不包括），即[low, high)。\n",
    "如果没有写参数high的值，则返回[0,low)的值。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:1    the loss:-5.388679\n",
      "iter_count:2    the loss:-0.903801\n",
      "iter_count:3    the loss:-0.212052\n",
      "iter_count:4    the loss:-0.104556\n",
      "iter_count:5    the loss:-0.087062\n",
      "iter_count:6    the loss:-0.083443\n",
      "iter_count:7    the loss:-0.081974\n",
      "iter_count:8    the loss:-0.080847\n",
      "iter_count:9    the loss:-0.079785\n",
      "iter_count:10    the loss:-0.078745\n",
      "iter_count:11    the loss:-0.077719\n",
      "iter_count:12    the loss:-0.076707\n",
      "iter_count:13    the loss:-0.075708\n",
      "iter_count:14    the loss:-0.074722\n",
      "iter_count:15    the loss:-0.073748\n",
      "iter_count:16    the loss:-0.072788\n",
      "iter_count:17    the loss:-0.071840\n",
      "iter_count:18    the loss:-0.070904\n",
      "iter_count:19    the loss:-0.069981\n",
      "iter_count:20    the loss:-0.069069\n",
      "iter_count:21    the loss:-0.068170\n",
      "iter_count:22    the loss:-0.067282\n",
      "iter_count:23    the loss:-0.066405\n",
      "iter_count:24    the loss:-0.065541\n",
      "iter_count:25    the loss:-0.064687\n",
      "iter_count:26    the loss:-0.063844\n",
      "iter_count:27    the loss:-0.063013\n",
      "iter_count:28    the loss:-0.062192\n",
      "iter_count:29    the loss:-0.061382\n",
      "iter_count:30    the loss:-0.060583\n",
      "iter_count:31    the loss:-0.059794\n",
      "iter_count:32    the loss:-0.059015\n",
      "iter_count:33    the loss:-0.058246\n",
      "iter_count:34    the loss:-0.057488\n",
      "iter_count:35    the loss:-0.056739\n",
      "iter_count:36    the loss:-0.056000\n",
      "iter_count:37    the loss:-0.055271\n",
      "iter_count:38    the loss:-0.054551\n",
      "iter_count:39    the loss:-0.053840\n",
      "iter_count:40    the loss:-0.053139\n",
      "iter_count:41    the loss:-0.052447\n",
      "iter_count:42    the loss:-0.051764\n",
      "iter_count:43    the loss:-0.051090\n",
      "iter_count:44    the loss:-0.050424\n",
      "iter_count:45    the loss:-0.049768\n",
      "iter_count:46    the loss:-0.049119\n",
      "iter_count:47    the loss:-0.048480\n",
      "iter_count:48    the loss:-0.047848\n",
      "iter_count:49    the loss:-0.047225\n",
      "iter_count:50    the loss:-0.046610\n",
      "iter_count:51    the loss:-0.046003\n",
      "iter_count:52    the loss:-0.045404\n",
      "iter_count:53    the loss:-0.044812\n",
      "iter_count:54    the loss:-0.044229\n",
      "iter_count:55    the loss:-0.043653\n",
      "iter_count:56    the loss:-0.043084\n",
      "iter_count:57    the loss:-0.042523\n",
      "iter_count:58    the loss:-0.041969\n",
      "iter_count:59    the loss:-0.041423\n",
      "iter_count:60    the loss:-0.040883\n",
      "iter_count:61    the loss:-0.040351\n",
      "iter_count:62    the loss:-0.039825\n",
      "iter_count:63    the loss:-0.039306\n",
      "iter_count:64    the loss:-0.038794\n",
      "iter_count:65    the loss:-0.038289\n",
      "iter_count:66    the loss:-0.037790\n",
      "iter_count:67    the loss:-0.037298\n",
      "iter_count:68    the loss:-0.036813\n",
      "iter_count:69    the loss:-0.036333\n",
      "iter_count:70    the loss:-0.035860\n",
      "iter_count:71    the loss:-0.035393\n",
      "iter_count:72    the loss:-0.034932\n",
      "iter_count:73    the loss:-0.034477\n",
      "iter_count:74    the loss:-0.034028\n",
      "iter_count:75    the loss:-0.033585\n",
      "iter_count:76    the loss:-0.033147\n",
      "iter_count:77    the loss:-0.032716\n",
      "iter_count:78    the loss:-0.032289\n",
      "iter_count:79    the loss:-0.031869\n",
      "iter_count:80    the loss:-0.031454\n",
      "iter_count:81    the loss:-0.031044\n",
      "iter_count:82    the loss:-0.030640\n",
      "iter_count:83    the loss:-0.030241\n",
      "iter_count:84    the loss:-0.029847\n",
      "iter_count:85    the loss:-0.029458\n",
      "iter_count:86    the loss:-0.029074\n",
      "iter_count:87    the loss:-0.028696\n",
      "iter_count:88    the loss:-0.028322\n",
      "iter_count:89    the loss:-0.027953\n",
      "iter_count:90    the loss:-0.027589\n",
      "iter_count:91    the loss:-0.027230\n",
      "iter_count:92    the loss:-0.026875\n",
      "iter_count:93    the loss:-0.026525\n",
      "iter_count:94    the loss:-0.026180\n",
      "iter_count:95    the loss:-0.025839\n",
      "iter_count:96    the loss:-0.025502\n",
      "iter_count:97    the loss:-0.025170\n",
      "iter_count:98    the loss:-0.024842\n",
      "iter_count:99    the loss:-0.024519\n",
      "iter_count:100    the loss:-0.024199\n",
      "iter_count:101    the loss:-0.023884\n",
      "iter_count:102    the loss:-0.023573\n",
      "iter_count:103    the loss:-0.023266\n",
      "iter_count:104    the loss:-0.022963\n",
      "iter_count:105    the loss:-0.022664\n",
      "iter_count:106    the loss:-0.022369\n",
      "iter_count:107    the loss:-0.022077\n",
      "iter_count:108    the loss:-0.021790\n",
      "iter_count:109    the loss:-0.021506\n",
      "iter_count:110    the loss:-0.021226\n",
      "iter_count:111    the loss:-0.020950\n",
      "iter_count:112    the loss:-0.020677\n",
      "iter_count:113    the loss:-0.020407\n",
      "iter_count:114    the loss:-0.020142\n",
      "iter_count:115    the loss:-0.019879\n",
      "iter_count:116    the loss:-0.019620\n",
      "iter_count:117    the loss:-0.019365\n",
      "iter_count:118    the loss:-0.019113\n",
      "iter_count:119    the loss:-0.018864\n",
      "iter_count:120    the loss:-0.018618\n",
      "iter_count:121    the loss:-0.018375\n",
      "iter_count:122    the loss:-0.018136\n",
      "iter_count:123    the loss:-0.017900\n",
      "iter_count:124    the loss:-0.017667\n",
      "iter_count:125    the loss:-0.017437\n",
      "iter_count:126    the loss:-0.017210\n",
      "iter_count:127    the loss:-0.016985\n",
      "iter_count:128    the loss:-0.016764\n",
      "iter_count:129    the loss:-0.016546\n",
      "iter_count:130    the loss:-0.016330\n",
      "iter_count:131    the loss:-0.016118\n",
      "iter_count:132    the loss:-0.015908\n",
      "iter_count:133    the loss:-0.015701\n",
      "iter_count:134    the loss:-0.015496\n",
      "iter_count:135    the loss:-0.015294\n",
      "iter_count:136    the loss:-0.015095\n",
      "iter_count:137    the loss:-0.014899\n",
      "iter_count:138    the loss:-0.014704\n",
      "iter_count:139    the loss:-0.014513\n",
      "iter_count:140    the loss:-0.014324\n",
      "iter_count:141    the loss:-0.014137\n",
      "iter_count:142    the loss:-0.013953\n",
      "iter_count:143    the loss:-0.013772\n",
      "iter_count:144    the loss:-0.013592\n",
      "iter_count:145    the loss:-0.013415\n",
      "iter_count:146    the loss:-0.013240\n",
      "iter_count:147    the loss:-0.013068\n",
      "iter_count:148    the loss:-0.012898\n",
      "iter_count:149    the loss:-0.012730\n",
      "iter_count:150    the loss:-0.012564\n",
      "iter_count:151    the loss:-0.012400\n",
      "iter_count:152    the loss:-0.012239\n",
      "iter_count:153    the loss:-0.012079\n",
      "iter_count:154    the loss:-0.011922\n",
      "iter_count:155    the loss:-0.011767\n",
      "iter_count:156    the loss:-0.011614\n",
      "iter_count:157    the loss:-0.011462\n",
      "iter_count:158    the loss:-0.011313\n",
      "iter_count:159    the loss:-0.011166\n",
      "iter_count:160    the loss:-0.011020\n",
      "iter_count:161    the loss:-0.010877\n",
      "iter_count:162    the loss:-0.010735\n",
      "iter_count:163    the loss:-0.010595\n",
      "iter_count:164    the loss:-0.010457\n",
      "iter_count:165    the loss:-0.010321\n",
      "iter_count:166    the loss:-0.010187\n",
      "iter_count:167    the loss:-0.010054\n",
      "iter_count:168    the loss:-0.009923\n",
      "iter_count:169    the loss:-0.009794\n",
      "iter_count:170    the loss:-0.009666\n",
      "iter_count:171    the loss:-0.009540\n",
      "iter_count:172    the loss:-0.009416\n",
      "iter_count:173    the loss:-0.009293\n",
      "iter_count:174    the loss:-0.009172\n",
      "iter_count:175    the loss:-0.009053\n",
      "iter_count:176    the loss:-0.008935\n",
      "iter_count:177    the loss:-0.008819\n",
      "iter_count:178    the loss:-0.008704\n",
      "iter_count:179    the loss:-0.008590\n",
      "iter_count:180    the loss:-0.008479\n",
      "iter_count:181    the loss:-0.008368\n",
      "iter_count:182    the loss:-0.008259\n",
      "iter_count:183    the loss:-0.008152\n",
      "iter_count:184    the loss:-0.008045\n",
      "iter_count:185    the loss:-0.007941\n",
      "iter_count:186    the loss:-0.007837\n",
      "iter_count:187    the loss:-0.007735\n",
      "iter_count:188    the loss:-0.007634\n",
      "iter_count:189    the loss:-0.007535\n",
      "iter_count:190    the loss:-0.007437\n",
      "iter_count:191    the loss:-0.007340\n",
      "iter_count:192    the loss:-0.007244\n",
      "iter_count:193    the loss:-0.007150\n",
      "iter_count:194    the loss:-0.007057\n",
      "iter_count:195    the loss:-0.006965\n",
      "iter_count:196    the loss:-0.006874\n",
      "iter_count:197    the loss:-0.006785\n",
      "iter_count:198    the loss:-0.006696\n",
      "iter_count:199    the loss:-0.006609\n",
      "iter_count:200    the loss:-0.006523\n",
      "iter_count:201    the loss:-0.006438\n",
      "iter_count:202    the loss:-0.006354\n",
      "iter_count:203    the loss:-0.006271\n",
      "iter_count:204    the loss:-0.006190\n",
      "iter_count:205    the loss:-0.006109\n",
      "iter_count:206    the loss:-0.006030\n",
      "iter_count:207    the loss:-0.005951\n",
      "iter_count:208    the loss:-0.005874\n",
      "iter_count:209    the loss:-0.005797\n",
      "iter_count:210    the loss:-0.005722\n",
      "iter_count:211    the loss:-0.005647\n",
      "iter_count:212    the loss:-0.005574\n",
      "iter_count:213    the loss:-0.005501\n",
      "iter_count:214    the loss:-0.005429\n",
      "iter_count:215    the loss:-0.005359\n",
      "iter_count:216    the loss:-0.005289\n",
      "iter_count:217    the loss:-0.005220\n",
      "iter_count:218    the loss:-0.005152\n",
      "iter_count:219    the loss:-0.005085\n",
      "iter_count:220    the loss:-0.005019\n",
      "iter_count:221    the loss:-0.004953\n",
      "iter_count:222    the loss:-0.004889\n",
      "iter_count:223    the loss:-0.004825\n",
      "iter_count:224    the loss:-0.004762\n",
      "iter_count:225    the loss:-0.004700\n",
      "iter_count:226    the loss:-0.004639\n",
      "iter_count:227    the loss:-0.004579\n",
      "iter_count:228    the loss:-0.004519\n",
      "iter_count:229    the loss:-0.004460\n",
      "iter_count:230    the loss:-0.004402\n",
      "iter_count:231    the loss:-0.004345\n",
      "iter_count:232    the loss:-0.004288\n",
      "iter_count:233    the loss:-0.004232\n",
      "iter_count:234    the loss:-0.004177\n",
      "iter_count:235    the loss:-0.004123\n",
      "iter_count:236    the loss:-0.004069\n",
      "iter_count:237    the loss:-0.004016\n",
      "iter_count:238    the loss:-0.003964\n",
      "iter_count:239    the loss:-0.003912\n",
      "iter_count:240    the loss:-0.003861\n",
      "iter_count:241    the loss:-0.003811\n",
      "iter_count:242    the loss:-0.003761\n",
      "iter_count:243    the loss:-0.003712\n",
      "iter_count:244    the loss:-0.003664\n",
      "iter_count:245    the loss:-0.003616\n",
      "iter_count:246    the loss:-0.003569\n",
      "iter_count:247    the loss:-0.003523\n",
      "iter_count:248    the loss:-0.003477\n",
      "iter_count:249    the loss:-0.003431\n",
      "iter_count:250    the loss:-0.003387\n",
      "iter_count:251    the loss:-0.003343\n",
      "iter_count:252    the loss:-0.003299\n",
      "iter_count:253    the loss:-0.003256\n",
      "iter_count:254    the loss:-0.003214\n",
      "iter_count:255    the loss:-0.003172\n",
      "iter_count:256    the loss:-0.003130\n",
      "iter_count:257    the loss:-0.003090\n",
      "iter_count:258    the loss:-0.003049\n",
      "iter_count:259    the loss:-0.003010\n",
      "iter_count:260    the loss:-0.002971\n",
      "iter_count:261    the loss:-0.002932\n",
      "iter_count:262    the loss:-0.002894\n",
      "iter_count:263    the loss:-0.002856\n",
      "iter_count:264    the loss:-0.002819\n",
      "iter_count:265    the loss:-0.002782\n",
      "iter_count:266    the loss:-0.002746\n",
      "iter_count:267    the loss:-0.002710\n",
      "iter_count:268    the loss:-0.002675\n",
      "iter_count:269    the loss:-0.002640\n",
      "iter_count:270    the loss:-0.002606\n",
      "iter_count:271    the loss:-0.002572\n",
      "iter_count:272    the loss:-0.002538\n",
      "iter_count:273    the loss:-0.002505\n",
      "iter_count:274    the loss:-0.002472\n",
      "iter_count:275    the loss:-0.002440\n",
      "iter_count:276    the loss:-0.002408\n",
      "iter_count:277    the loss:-0.002377\n",
      "iter_count:278    the loss:-0.002346\n",
      "iter_count:279    the loss:-0.002316\n",
      "iter_count:280    the loss:-0.002285\n",
      "iter_count:281    the loss:-0.002256\n",
      "iter_count:282    the loss:-0.002226\n",
      "iter_count:283    the loss:-0.002197\n",
      "iter_count:284    the loss:-0.002169\n",
      "iter_count:285    the loss:-0.002140\n",
      "iter_count:286    the loss:-0.002113\n",
      "iter_count:287    the loss:-0.002085\n",
      "iter_count:288    the loss:-0.002058\n",
      "iter_count:289    the loss:-0.002031\n",
      "iter_count:290    the loss:-0.002005\n",
      "iter_count:291    the loss:-0.001979\n",
      "iter_count:292    the loss:-0.001953\n",
      "iter_count:293    the loss:-0.001927\n",
      "iter_count:294    the loss:-0.001902\n",
      "iter_count:295    the loss:-0.001877\n",
      "iter_count:296    the loss:-0.001853\n",
      "iter_count:297    the loss:-0.001829\n",
      "iter_count:298    the loss:-0.001805\n",
      "iter_count:299    the loss:-0.001782\n",
      "iter_count:300    the loss:-0.001758\n",
      "iter_count:301    the loss:-0.001735\n",
      "iter_count:302    the loss:-0.001713\n",
      "iter_count:303    the loss:-0.001691\n",
      "iter_count:304    the loss:-0.001668\n",
      "iter_count:305    the loss:-0.001647\n",
      "iter_count:306    the loss:-0.001625\n",
      "iter_count:307    the loss:-0.001604\n",
      "iter_count:308    the loss:-0.001583\n",
      "iter_count:309    the loss:-0.001563\n",
      "iter_count:310    the loss:-0.001542\n",
      "iter_count:311    the loss:-0.001522\n",
      "iter_count:312    the loss:-0.001502\n",
      "iter_count:313    the loss:-0.001483\n",
      "iter_count:314    the loss:-0.001463\n",
      "iter_count:315    the loss:-0.001444\n",
      "iter_count:316    the loss:-0.001426\n",
      "iter_count:317    the loss:-0.001407\n",
      "iter_count:318    the loss:-0.001389\n",
      "iter_count:319    the loss:-0.001371\n",
      "iter_count:320    the loss:-0.001353\n",
      "iter_count:321    the loss:-0.001335\n",
      "iter_count:322    the loss:-0.001318\n",
      "iter_count:323    the loss:-0.001301\n",
      "iter_count:324    the loss:-0.001284\n",
      "iter_count:325    the loss:-0.001267\n",
      "iter_count:326    the loss:-0.001250\n",
      "iter_count:327    the loss:-0.001234\n",
      "iter_count:328    the loss:-0.001218\n",
      "iter_count:329    the loss:-0.001202\n",
      "iter_count:330    the loss:-0.001187\n",
      "iter_count:331    the loss:-0.001171\n",
      "iter_count:332    the loss:-0.001156\n",
      "iter_count:333    the loss:-0.001141\n",
      "iter_count:334    the loss:-0.001126\n",
      "iter_count:335    the loss:-0.001111\n",
      "iter_count:336    the loss:-0.001097\n",
      "iter_count:337    the loss:-0.001083\n",
      "iter_count:338    the loss:-0.001068\n",
      "iter_count:339    the loss:-0.001055\n",
      "iter_count:340    the loss:-0.001041\n",
      "iter_count:341    the loss:-0.001027\n",
      "iter_count:342    the loss:-0.001014\n",
      "iter_count:343    the loss:-0.001001\n",
      "iter_count:344    the loss:-0.000988\n",
      "iter_count:345    the loss:-0.000975\n",
      "iter_count:346    the loss:-0.000962\n",
      "iter_count:347    the loss:-0.000950\n",
      "iter_count:348    the loss:-0.000937\n",
      "iter_count:349    the loss:-0.000925\n",
      "iter_count:350    the loss:-0.000913\n",
      "iter_count:351    the loss:-0.000901\n",
      "iter_count:352    the loss:-0.000889\n",
      "iter_count:353    the loss:-0.000878\n",
      "iter_count:354    the loss:-0.000866\n",
      "iter_count:355    the loss:-0.000855\n",
      "iter_count:356    the loss:-0.000844\n",
      "iter_count:357    the loss:-0.000833\n",
      "iter_count:358    the loss:-0.000822\n",
      "iter_count:359    the loss:-0.000811\n",
      "iter_count:360    the loss:-0.000801\n",
      "iter_count:361    the loss:-0.000790\n",
      "iter_count:362    the loss:-0.000780\n",
      "iter_count:363    the loss:-0.000770\n",
      "iter_count:364    the loss:-0.000760\n",
      "iter_count:365    the loss:-0.000750\n",
      "iter_count:366    the loss:-0.000740\n",
      "iter_count:367    the loss:-0.000731\n",
      "iter_count:368    the loss:-0.000721\n",
      "iter_count:369    the loss:-0.000712\n",
      "iter_count:370    the loss:-0.000702\n",
      "iter_count:371    the loss:-0.000693\n",
      "iter_count:372    the loss:-0.000684\n",
      "iter_count:373    the loss:-0.000675\n",
      "iter_count:374    the loss:-0.000666\n",
      "iter_count:375    the loss:-0.000658\n",
      "iter_count:376    the loss:-0.000649\n",
      "iter_count:377    the loss:-0.000641\n",
      "iter_count:378    the loss:-0.000632\n",
      "iter_count:379    the loss:-0.000624\n",
      "iter_count:380    the loss:-0.000616\n",
      "iter_count:381    the loss:-0.000608\n",
      "iter_count:382    the loss:-0.000600\n",
      "iter_count:383    the loss:-0.000592\n",
      "iter_count:384    the loss:-0.000585\n",
      "iter_count:385    the loss:-0.000577\n",
      "iter_count:386    the loss:-0.000569\n",
      "iter_count:387    the loss:-0.000562\n",
      "iter_count:388    the loss:-0.000555\n",
      "iter_count:389    the loss:-0.000547\n",
      "iter_count:390    the loss:-0.000540\n",
      "iter_count:391    the loss:-0.000533\n",
      "iter_count:392    the loss:-0.000526\n",
      "iter_count:393    the loss:-0.000520\n",
      "iter_count:394    the loss:-0.000513\n",
      "iter_count:395    the loss:-0.000506\n",
      "iter_count:396    the loss:-0.000499\n",
      "iter_count:397    the loss:-0.000493\n",
      "iter_count:398    the loss:-0.000487\n",
      "iter_count:399    the loss:-0.000480\n",
      "iter_count:400    the loss:-0.000474\n",
      "iter_count:401    the loss:-0.000468\n",
      "iter_count:402    the loss:-0.000462\n",
      "iter_count:403    the loss:-0.000456\n",
      "iter_count:404    the loss:-0.000450\n",
      "iter_count:405    the loss:-0.000444\n",
      "iter_count:406    the loss:-0.000438\n",
      "iter_count:407    the loss:-0.000432\n",
      "iter_count:408    the loss:-0.000427\n",
      "iter_count:409    the loss:-0.000421\n",
      "iter_count:410    the loss:-0.000416\n",
      "iter_count:411    the loss:-0.000410\n",
      "iter_count:412    the loss:-0.000405\n",
      "iter_count:413    the loss:-0.000400\n",
      "iter_count:414    the loss:-0.000394\n",
      "iter_count:415    the loss:-0.000389\n",
      "iter_count:416    the loss:-0.000384\n",
      "iter_count:417    the loss:-0.000379\n",
      "iter_count:418    the loss:-0.000374\n",
      "iter_count:419    the loss:-0.000369\n",
      "iter_count:420    the loss:-0.000365\n",
      "iter_count:421    the loss:-0.000360\n",
      "iter_count:422    the loss:-0.000355\n",
      "iter_count:423    the loss:-0.000351\n",
      "iter_count:424    the loss:-0.000346\n",
      "iter_count:425    the loss:-0.000342\n",
      "iter_count:426    the loss:-0.000337\n",
      "iter_count:427    the loss:-0.000333\n",
      "iter_count:428    the loss:-0.000328\n",
      "iter_count:429    the loss:-0.000324\n",
      "iter_count:430    the loss:-0.000320\n",
      "iter_count:431    the loss:-0.000316\n",
      "iter_count:432    the loss:-0.000312\n",
      "iter_count:433    the loss:-0.000308\n",
      "iter_count:434    the loss:-0.000304\n",
      "iter_count:435    the loss:-0.000300\n",
      "iter_count:436    the loss:-0.000296\n",
      "iter_count:437    the loss:-0.000292\n",
      "iter_count:438    the loss:-0.000288\n",
      "iter_count:439    the loss:-0.000284\n",
      "iter_count:440    the loss:-0.000281\n",
      "iter_count:441    the loss:-0.000277\n",
      "iter_count:442    the loss:-0.000273\n",
      "iter_count:443    the loss:-0.000270\n",
      "iter_count:444    the loss:-0.000266\n",
      "iter_count:445    the loss:-0.000263\n",
      "iter_count:446    the loss:-0.000259\n",
      "iter_count:447    the loss:-0.000256\n",
      "iter_count:448    the loss:-0.000253\n",
      "iter_count:449    the loss:-0.000249\n",
      "iter_count:450    the loss:-0.000246\n",
      "iter_count:451    the loss:-0.000243\n",
      "iter_count:452    the loss:-0.000240\n",
      "iter_count:453    the loss:-0.000237\n",
      "iter_count:454    the loss:-0.000234\n",
      "iter_count:455    the loss:-0.000230\n",
      "iter_count:456    the loss:-0.000227\n",
      "iter_count:457    the loss:-0.000224\n",
      "iter_count:458    the loss:-0.000222\n",
      "iter_count:459    the loss:-0.000219\n",
      "iter_count:460    the loss:-0.000216\n",
      "iter_count:461    the loss:-0.000213\n",
      "iter_count:462    the loss:-0.000210\n",
      "iter_count:463    the loss:-0.000208\n",
      "iter_count:464    the loss:-0.000205\n",
      "iter_count:465    the loss:-0.000202\n",
      "iter_count:466    the loss:-0.000200\n",
      "iter_count:467    the loss:-0.000197\n",
      "iter_count:468    the loss:-0.000194\n",
      "iter_count:469    the loss:-0.000192\n",
      "iter_count:470    the loss:-0.000189\n",
      "iter_count:471    the loss:-0.000187\n",
      "iter_count:472    the loss:-0.000184\n",
      "iter_count:473    the loss:-0.000182\n",
      "iter_count:474    the loss:-0.000180\n",
      "iter_count:475    the loss:-0.000177\n",
      "iter_count:476    the loss:-0.000175\n",
      "iter_count:477    the loss:-0.000173\n",
      "iter_count:478    the loss:-0.000170\n",
      "iter_count:479    the loss:-0.000168\n",
      "iter_count:480    the loss:-0.000166\n",
      "iter_count:481    the loss:-0.000164\n",
      "iter_count:482    the loss:-0.000162\n",
      "iter_count:483    the loss:-0.000160\n",
      "iter_count:484    the loss:-0.000158\n",
      "iter_count:485    the loss:-0.000156\n",
      "iter_count:486    the loss:-0.000153\n",
      "iter_count:487    the loss:-0.000151\n",
      "iter_count:488    the loss:-0.000150\n",
      "iter_count:489    the loss:-0.000148\n",
      "iter_count:490    the loss:-0.000146\n",
      "iter_count:491    the loss:-0.000144\n",
      "iter_count:492    the loss:-0.000142\n",
      "iter_count:493    the loss:-0.000140\n",
      "iter_count:494    the loss:-0.000138\n",
      "iter_count:495    the loss:-0.000136\n",
      "iter_count:496    the loss:-0.000135\n",
      "iter_count:497    the loss:-0.000133\n",
      "iter_count:498    the loss:-0.000131\n",
      "iter_count:499    the loss:-0.000129\n",
      "iter_count:500    the loss:-0.000128\n",
      "iter_count:501    the loss:-0.000126\n",
      "iter_count:502    the loss:-0.000124\n",
      "iter_count:503    the loss:-0.000123\n",
      "iter_count:504    the loss:-0.000121\n",
      "iter_count:505    the loss:-0.000120\n",
      "iter_count:506    the loss:-0.000118\n",
      "iter_count:507    the loss:-0.000117\n",
      "iter_count:508    the loss:-0.000115\n",
      "iter_count:509    the loss:-0.000114\n",
      "iter_count:510    the loss:-0.000112\n",
      "iter_count:511    the loss:-0.000111\n",
      "iter_count:512    the loss:-0.000109\n",
      "iter_count:513    the loss:-0.000108\n",
      "iter_count:514    the loss:-0.000106\n",
      "iter_count:515    the loss:-0.000105\n",
      "iter_count:516    the loss:-0.000104\n",
      "iter_count:517    the loss:-0.000102\n",
      "iter_count:518    the loss:-0.000101\n",
      "iter_count:519    the loss:-0.000100\n",
      "[3.00027285 3.99984383]\n",
      "iter_count:1    the loss:-2810.646015\n",
      "iter_count:2    the loss:-105.175574\n",
      "iter_count:3    the loss:-0.023662\n",
      "iter_count:4    the loss:2.562723\n",
      "iter_count:5    the loss:-11.281977\n",
      "iter_count:6    the loss:-6.308893\n",
      "iter_count:7    the loss:-3.558781\n",
      "iter_count:8    the loss:-2.919275\n",
      "iter_count:9    the loss:1.953772\n",
      "iter_count:10    the loss:-0.896478\n",
      "iter_count:11    the loss:1.114978\n",
      "iter_count:12    the loss:3.190756\n",
      "iter_count:13    the loss:7.486459\n",
      "iter_count:14    the loss:4.327120\n",
      "iter_count:15    the loss:9.117516\n",
      "iter_count:16    the loss:5.555935\n",
      "iter_count:17    the loss:9.933926\n",
      "iter_count:18    the loss:3.176063\n",
      "iter_count:19    the loss:-8.280531\n",
      "iter_count:20    the loss:-0.686030\n",
      "iter_count:21    the loss:-3.521416\n",
      "iter_count:22    the loss:-3.184932\n",
      "iter_count:23    the loss:-3.580618\n",
      "iter_count:24    the loss:1.606604\n",
      "iter_count:25    the loss:5.775565\n",
      "iter_count:26    the loss:9.605820\n",
      "iter_count:27    the loss:-9.034407\n",
      "iter_count:28    the loss:-3.356271\n",
      "iter_count:29    the loss:1.287964\n",
      "iter_count:30    the loss:-5.599566\n",
      "iter_count:31    the loss:-5.844116\n",
      "iter_count:32    the loss:-0.275019\n",
      "iter_count:33    the loss:3.537691\n",
      "iter_count:34    the loss:4.695141\n",
      "iter_count:35    the loss:-4.765140\n",
      "iter_count:36    the loss:-4.468594\n",
      "iter_count:37    the loss:-0.105439\n",
      "iter_count:38    the loss:-0.390241\n",
      "iter_count:39    the loss:-1.328639\n",
      "iter_count:40    the loss:2.111038\n",
      "iter_count:41    the loss:5.005776\n",
      "iter_count:42    the loss:-0.069864\n",
      "iter_count:43    the loss:1.545542\n",
      "iter_count:44    the loss:4.015511\n",
      "iter_count:45    the loss:3.994292\n",
      "iter_count:46    the loss:0.591348\n",
      "iter_count:47    the loss:-5.324055\n",
      "iter_count:48    the loss:-3.517990\n",
      "iter_count:49    the loss:-0.743839\n",
      "iter_count:50    the loss:1.459532\n",
      "iter_count:51    the loss:3.074947\n",
      "iter_count:52    the loss:4.186965\n",
      "iter_count:53    the loss:5.691296\n",
      "iter_count:54    the loss:-0.831158\n",
      "iter_count:55    the loss:1.995481\n",
      "iter_count:56    the loss:-4.411325\n",
      "iter_count:57    the loss:-0.852941\n",
      "iter_count:58    the loss:1.113981\n",
      "iter_count:59    the loss:2.455163\n",
      "iter_count:60    the loss:3.874231\n",
      "iter_count:61    the loss:5.365774\n",
      "iter_count:62    the loss:6.658788\n",
      "iter_count:63    the loss:-3.495316\n",
      "iter_count:64    the loss:-1.313115\n",
      "iter_count:65    the loss:-2.086081\n",
      "iter_count:66    the loss:-0.223460\n",
      "iter_count:67    the loss:0.988453\n",
      "iter_count:68    the loss:0.914306\n",
      "iter_count:69    the loss:-0.663472\n",
      "iter_count:70    the loss:-1.889153\n",
      "iter_count:71    the loss:-0.269542\n",
      "iter_count:72    the loss:0.709541\n",
      "iter_count:73    the loss:1.222493\n",
      "iter_count:74    the loss:0.542222\n",
      "iter_count:75    the loss:-1.638269\n",
      "iter_count:76    the loss:-1.083193\n",
      "iter_count:77    the loss:-0.466288\n",
      "iter_count:78    the loss:-1.572138\n",
      "iter_count:79    the loss:-0.230611\n",
      "iter_count:80    the loss:0.621060\n",
      "iter_count:81    the loss:-0.838222\n",
      "iter_count:82    the loss:-0.094592\n",
      "iter_count:83    the loss:0.235180\n",
      "iter_count:84    the loss:-0.231466\n",
      "iter_count:85    the loss:0.211249\n",
      "iter_count:86    the loss:0.575389\n",
      "iter_count:87    the loss:0.945573\n",
      "iter_count:88    the loss:1.544147\n",
      "iter_count:89    the loss:1.986484\n",
      "iter_count:90    the loss:2.414829\n",
      "iter_count:91    the loss:1.147666\n",
      "iter_count:92    the loss:-1.247793\n",
      "iter_count:93    the loss:-0.649857\n",
      "iter_count:94    the loss:0.128980\n",
      "iter_count:95    the loss:-0.966048\n",
      "iter_count:96    the loss:-0.238111\n",
      "iter_count:97    the loss:-0.903207\n",
      "iter_count:98    the loss:-0.222102\n",
      "iter_count:99    the loss:-0.843861\n",
      "iter_count:100    the loss:-0.740774\n",
      "iter_count:101    the loss:-0.123230\n",
      "iter_count:102    the loss:-0.089852\n",
      "iter_count:103    the loss:0.338653\n",
      "iter_count:104    the loss:-0.347395\n",
      "iter_count:105    the loss:0.161539\n",
      "iter_count:106    the loss:0.305507\n",
      "iter_count:107    the loss:-0.220145\n",
      "iter_count:108    the loss:-0.343965\n",
      "iter_count:109    the loss:0.146074\n",
      "iter_count:110    the loss:-0.546924\n",
      "iter_count:111    the loss:-0.027461\n",
      "iter_count:112    the loss:0.347212\n",
      "iter_count:113    the loss:-0.267172\n",
      "iter_count:114    the loss:-0.306610\n",
      "iter_count:115    the loss:-0.292572\n",
      "iter_count:116    the loss:0.118124\n",
      "iter_count:117    the loss:-0.134955\n",
      "iter_count:118    the loss:-0.226901\n",
      "iter_count:119    the loss:-0.437202\n",
      "iter_count:120    the loss:-0.014562\n",
      "iter_count:121    the loss:-0.431705\n",
      "iter_count:122    the loss:-0.329059\n",
      "iter_count:123    the loss:-0.499710\n",
      "iter_count:124    the loss:-0.113472\n",
      "iter_count:125    the loss:0.212598\n",
      "iter_count:126    the loss:-0.105315\n",
      "iter_count:127    the loss:-0.193310\n",
      "iter_count:128    the loss:0.121870\n",
      "iter_count:129    the loss:0.033993\n",
      "iter_count:130    the loss:0.039064\n",
      "iter_count:131    the loss:-0.481548\n",
      "iter_count:132    the loss:-0.077396\n",
      "iter_count:133    the loss:-0.302686\n",
      "iter_count:134    the loss:0.018775\n",
      "iter_count:135    the loss:-0.166379\n",
      "iter_count:136    the loss:0.108942\n",
      "iter_count:137    the loss:0.107184\n",
      "iter_count:138    the loss:-0.123982\n",
      "iter_count:139    the loss:-0.056326\n",
      "iter_count:140    the loss:-0.063733\n",
      "iter_count:141    the loss:0.142774\n",
      "iter_count:142    the loss:-0.379150\n",
      "iter_count:143    the loss:-0.105678\n",
      "iter_count:144    the loss:-0.310574\n",
      "iter_count:145    the loss:-0.048084\n",
      "iter_count:146    the loss:-0.219146\n",
      "iter_count:147    the loss:-0.069186\n",
      "iter_count:148    the loss:0.098328\n",
      "iter_count:149    the loss:0.233705\n",
      "iter_count:150    the loss:0.364879\n",
      "iter_count:151    the loss:-0.210280\n",
      "iter_count:152    the loss:-0.011631\n",
      "iter_count:153    the loss:0.123284\n",
      "iter_count:154    the loss:-0.175864\n",
      "iter_count:155    the loss:0.014534\n",
      "iter_count:156    the loss:-0.234416\n",
      "iter_count:157    the loss:-0.087396\n",
      "iter_count:158    the loss:-0.079233\n",
      "iter_count:159    the loss:0.055879\n",
      "iter_count:160    the loss:-0.202444\n",
      "iter_count:161    the loss:-0.171224\n",
      "iter_count:162    the loss:-0.035621\n",
      "iter_count:163    the loss:-0.127866\n",
      "iter_count:164    the loss:-0.010101\n",
      "iter_count:165    the loss:0.083647\n",
      "iter_count:166    the loss:-0.136241\n",
      "iter_count:167    the loss:-0.014499\n",
      "iter_count:168    the loss:0.075495\n",
      "iter_count:169    the loss:0.142128\n",
      "iter_count:170    the loss:0.062295\n",
      "iter_count:171    the loss:0.081355\n",
      "iter_count:172    the loss:0.150014\n",
      "iter_count:173    the loss:0.169403\n",
      "iter_count:174    the loss:0.200690\n",
      "iter_count:175    the loss:0.236297\n",
      "iter_count:176    the loss:-0.084823\n",
      "iter_count:177    the loss:-0.050528\n",
      "iter_count:178    the loss:0.033196\n",
      "iter_count:179    the loss:0.036493\n",
      "iter_count:180    the loss:0.101338\n",
      "iter_count:181    the loss:-0.011548\n",
      "iter_count:182    the loss:0.026957\n",
      "iter_count:183    the loss:0.030207\n",
      "iter_count:184    the loss:-0.113908\n",
      "iter_count:185    the loss:-0.089201\n",
      "iter_count:186    the loss:-0.048018\n",
      "iter_count:187    the loss:-0.099057\n",
      "iter_count:188    the loss:-0.036740\n",
      "iter_count:189    the loss:-0.011378\n",
      "iter_count:190    the loss:0.051731\n",
      "iter_count:191    the loss:-0.086693\n",
      "iter_count:192    the loss:-0.004655\n",
      "iter_count:193    the loss:-0.072112\n",
      "iter_count:194    the loss:0.001499\n",
      "iter_count:195    the loss:0.001459\n",
      "iter_count:196    the loss:-0.084692\n",
      "iter_count:197    the loss:-0.067181\n",
      "iter_count:198    the loss:-0.010217\n",
      "iter_count:199    the loss:0.014751\n",
      "iter_count:200    the loss:-0.014361\n",
      "iter_count:201    the loss:0.034957\n",
      "iter_count:202    the loss:0.074476\n",
      "iter_count:203    the loss:0.033059\n",
      "iter_count:204    the loss:-0.078281\n",
      "iter_count:205    the loss:-0.014589\n",
      "iter_count:206    the loss:0.027724\n",
      "iter_count:207    the loss:0.037036\n",
      "iter_count:208    the loss:0.061799\n",
      "iter_count:209    the loss:-0.066382\n",
      "iter_count:210    the loss:-0.014330\n",
      "iter_count:211    the loss:0.009508\n",
      "iter_count:212    the loss:-0.035908\n",
      "iter_count:213    the loss:0.004107\n",
      "iter_count:214    the loss:-0.043034\n",
      "iter_count:215    the loss:-0.026951\n",
      "iter_count:216    the loss:0.008082\n",
      "iter_count:217    the loss:-0.006868\n",
      "iter_count:218    the loss:0.019870\n",
      "iter_count:219    the loss:0.040689\n",
      "iter_count:220    the loss:-0.045192\n",
      "iter_count:221    the loss:-0.013354\n",
      "iter_count:222    the loss:-0.034700\n",
      "iter_count:223    the loss:-0.010231\n",
      "iter_count:224    the loss:-0.021822\n",
      "iter_count:225    the loss:-0.001226\n",
      "iter_count:226    the loss:0.012651\n",
      "iter_count:227    the loss:0.021591\n",
      "iter_count:228    the loss:0.023002\n",
      "iter_count:229    the loss:-0.000213\n",
      "iter_count:230    the loss:0.014425\n",
      "iter_count:231    the loss:-0.027711\n",
      "iter_count:232    the loss:-0.010062\n",
      "iter_count:233    the loss:-0.020766\n",
      "iter_count:234    the loss:-0.011590\n",
      "iter_count:235    the loss:0.005495\n",
      "iter_count:236    the loss:-0.015485\n",
      "iter_count:237    the loss:0.005439\n",
      "iter_count:238    the loss:0.021112\n",
      "iter_count:239    the loss:0.009157\n",
      "iter_count:240    the loss:0.021322\n",
      "iter_count:241    the loss:0.007214\n",
      "iter_count:242    the loss:0.002789\n",
      "iter_count:243    the loss:0.013703\n",
      "iter_count:244    the loss:0.026413\n",
      "iter_count:245    the loss:0.004253\n",
      "iter_count:246    the loss:-0.013755\n",
      "iter_count:247    the loss:-0.024740\n",
      "iter_count:248    the loss:-0.017503\n",
      "iter_count:249    the loss:-0.013207\n",
      "iter_count:250    the loss:-0.025044\n",
      "iter_count:251    the loss:-0.006187\n",
      "iter_count:252    the loss:0.008636\n",
      "iter_count:253    the loss:0.004416\n",
      "iter_count:254    the loss:-0.018233\n",
      "iter_count:255    the loss:-0.015893\n",
      "iter_count:256    the loss:0.000047\n",
      "iter_count:257    the loss:0.013257\n",
      "iter_count:258    the loss:-0.018664\n",
      "iter_count:259    the loss:-0.005592\n",
      "iter_count:260    the loss:0.002565\n",
      "iter_count:261    the loss:-0.018823\n",
      "iter_count:262    the loss:-0.005924\n",
      "iter_count:263    the loss:-0.005851\n",
      "iter_count:264    the loss:-0.014684\n",
      "iter_count:265    the loss:-0.010921\n",
      "iter_count:266    the loss:0.002263\n",
      "iter_count:267    the loss:0.011771\n",
      "iter_count:268    the loss:-0.011764\n",
      "iter_count:269    the loss:-0.000288\n",
      "iter_count:270    the loss:-0.014889\n",
      "iter_count:271    the loss:-0.002791\n",
      "iter_count:272    the loss:0.000374\n",
      "iter_count:273    the loss:-0.008998\n",
      "iter_count:274    the loss:0.000386\n",
      "iter_count:275    the loss:-0.009474\n",
      "iter_count:276    the loss:-0.000974\n",
      "iter_count:277    the loss:-0.006484\n",
      "iter_count:278    the loss:0.001750\n",
      "iter_count:279    the loss:-0.005424\n",
      "iter_count:280    the loss:-0.004599\n",
      "iter_count:281    the loss:-0.007110\n",
      "iter_count:282    the loss:0.000501\n",
      "iter_count:283    the loss:0.002735\n",
      "iter_count:284    the loss:0.008452\n",
      "iter_count:285    the loss:0.006655\n",
      "iter_count:286    the loss:0.011945\n",
      "iter_count:287    the loss:0.012570\n",
      "iter_count:288    the loss:0.013236\n",
      "iter_count:289    the loss:0.008938\n",
      "iter_count:290    the loss:0.002984\n",
      "iter_count:291    the loss:-0.004079\n",
      "iter_count:292    the loss:0.003181\n",
      "iter_count:293    the loss:-0.008366\n",
      "iter_count:294    the loss:-0.001043\n",
      "iter_count:295    the loss:-0.008681\n",
      "iter_count:296    the loss:-0.003424\n",
      "iter_count:297    the loss:-0.008263\n",
      "iter_count:298    the loss:-0.006076\n",
      "iter_count:299    the loss:-0.000665\n",
      "iter_count:300    the loss:0.003931\n",
      "iter_count:301    the loss:-0.002351\n",
      "iter_count:302    the loss:0.000905\n",
      "iter_count:303    the loss:0.003337\n",
      "iter_count:304    the loss:0.007493\n",
      "iter_count:305    the loss:0.010578\n",
      "iter_count:306    the loss:0.007609\n",
      "iter_count:307    the loss:-0.005906\n",
      "iter_count:308    the loss:-0.007150\n",
      "iter_count:309    the loss:-0.001045\n",
      "iter_count:310    the loss:0.003405\n",
      "iter_count:311    the loss:-0.006202\n",
      "iter_count:312    the loss:-0.002808\n",
      "iter_count:313    the loss:-0.004461\n",
      "iter_count:314    the loss:0.000573\n",
      "iter_count:315    the loss:0.004181\n",
      "iter_count:316    the loss:-0.005767\n",
      "iter_count:317    the loss:-0.001673\n",
      "iter_count:318    the loss:0.000308\n",
      "iter_count:319    the loss:-0.000142\n",
      "iter_count:320    the loss:0.001672\n",
      "iter_count:321    the loss:-0.001495\n",
      "iter_count:322    the loss:-0.002304\n",
      "iter_count:323    the loss:0.001315\n",
      "iter_count:324    the loss:-0.002793\n",
      "iter_count:325    the loss:-0.003864\n",
      "iter_count:326    the loss:-0.000446\n",
      "iter_count:327    the loss:-0.004245\n",
      "iter_count:328    the loss:-0.002585\n",
      "iter_count:329    the loss:-0.000113\n",
      "iter_count:330    the loss:0.002498\n",
      "iter_count:331    the loss:0.001485\n",
      "iter_count:332    the loss:0.003709\n",
      "iter_count:333    the loss:0.005441\n",
      "iter_count:334    the loss:-0.001243\n",
      "iter_count:335    the loss:-0.002131\n",
      "iter_count:336    the loss:0.000743\n",
      "iter_count:337    the loss:-0.003731\n",
      "iter_count:338    the loss:-0.001850\n",
      "iter_count:339    the loss:-0.000080\n",
      "iter_count:340    the loss:0.001376\n",
      "iter_count:341    the loss:-0.002247\n",
      "iter_count:342    the loss:-0.001647\n",
      "iter_count:343    the loss:0.000773\n",
      "iter_count:344    the loss:0.001712\n",
      "iter_count:345    the loss:0.000965\n",
      "iter_count:346    the loss:-0.000844\n",
      "iter_count:347    the loss:-0.001527\n",
      "iter_count:348    the loss:0.000182\n",
      "iter_count:349    the loss:0.001598\n",
      "iter_count:350    the loss:0.000614\n",
      "iter_count:351    the loss:-0.003218\n",
      "iter_count:352    the loss:-0.000985\n",
      "iter_count:353    the loss:-0.001824\n",
      "iter_count:354    the loss:-0.000055\n",
      "iter_count:355    the loss:-0.002003\n",
      "iter_count:356    the loss:-0.002239\n",
      "iter_count:357    the loss:-0.002098\n",
      "iter_count:358    the loss:-0.001809\n",
      "iter_count:359    the loss:-0.000993\n",
      "iter_count:360    the loss:-0.000494\n",
      "iter_count:361    the loss:-0.000137\n",
      "iter_count:362    the loss:0.001190\n",
      "iter_count:363    the loss:0.002152\n",
      "iter_count:364    the loss:-0.001859\n",
      "iter_count:365    the loss:-0.000357\n",
      "iter_count:366    the loss:0.000425\n",
      "iter_count:367    the loss:0.000064\n",
      "iter_count:368    the loss:0.001251\n",
      "iter_count:369    the loss:0.000854\n",
      "iter_count:370    the loss:-0.000239\n",
      "iter_count:371    the loss:0.000986\n",
      "iter_count:372    the loss:-0.000666\n",
      "iter_count:373    the loss:0.000619\n",
      "iter_count:374    the loss:0.000223\n",
      "iter_count:375    the loss:0.001225\n",
      "iter_count:376    the loss:-0.001676\n",
      "iter_count:377    the loss:-0.000151\n",
      "iter_count:378    the loss:0.000356\n",
      "iter_count:379    the loss:-0.000776\n",
      "iter_count:380    the loss:-0.001496\n",
      "iter_count:381    the loss:-0.000110\n",
      "iter_count:382    the loss:-0.000359\n",
      "iter_count:383    the loss:-0.001362\n",
      "iter_count:384    the loss:-0.000099\n",
      "iter_count:385    the loss:0.000758\n",
      "iter_count:386    the loss:-0.001344\n",
      "iter_count:387    the loss:-0.001015\n",
      "iter_count:388    the loss:-0.001107\n",
      "iter_count:389    the loss:-0.000575\n",
      "iter_count:390    the loss:-0.001133\n",
      "iter_count:391    the loss:-0.000104\n",
      "iter_count:392    the loss:-0.000315\n",
      "iter_count:393    the loss:0.000469\n",
      "iter_count:394    the loss:-0.000405\n",
      "iter_count:395    the loss:0.000286\n",
      "iter_count:396    the loss:-0.000086\n",
      "iter_count:397    the loss:-0.000258\n",
      "iter_count:398    the loss:0.000233\n",
      "iter_count:399    the loss:-0.000125\n",
      "iter_count:400    the loss:-0.000204\n",
      "iter_count:401    the loss:0.000437\n",
      "iter_count:402    the loss:-0.000260\n",
      "iter_count:403    the loss:0.000367\n",
      "iter_count:404    the loss:0.000387\n",
      "iter_count:405    the loss:-0.000312\n",
      "iter_count:406    the loss:-0.000930\n",
      "iter_count:407    the loss:-0.000090\n",
      "iter_count:408    the loss:-0.000053\n",
      "iter_count:409    the loss:0.000087\n",
      "iter_count:410    the loss:0.000150\n",
      "iter_count:411    the loss:0.000262\n",
      "iter_count:412    the loss:0.000744\n",
      "iter_count:413    the loss:-0.000880\n",
      "iter_count:414    the loss:-0.000132\n",
      "iter_count:415    the loss:-0.000530\n",
      "iter_count:416    the loss:-0.000303\n",
      "iter_count:417    the loss:-0.000298\n",
      "iter_count:418    the loss:-0.000166\n",
      "iter_count:419    the loss:-0.000312\n",
      "iter_count:420    the loss:-0.000314\n",
      "iter_count:421    the loss:-0.000281\n",
      "iter_count:422    the loss:-0.000280\n",
      "iter_count:423    the loss:-0.000494\n",
      "iter_count:424    the loss:-0.000487\n",
      "iter_count:425    the loss:-0.000058\n",
      "iter_count:426    the loss:-0.000058\n",
      "iter_count:427    the loss:0.000385\n",
      "iter_count:428    the loss:-0.000716\n",
      "iter_count:429    the loss:-0.000247\n",
      "iter_count:430    the loss:0.000108\n",
      "iter_count:431    the loss:0.000257\n",
      "iter_count:432    the loss:-0.000532\n",
      "iter_count:433    the loss:-0.000063\n",
      "iter_count:434    the loss:0.000211\n",
      "iter_count:435    the loss:-0.000183\n",
      "iter_count:436    the loss:-0.000171\n",
      "iter_count:437    the loss:0.000153\n",
      "iter_count:438    the loss:-0.000458\n",
      "iter_count:439    the loss:-0.000073\n",
      "iter_count:440    the loss:0.000186\n",
      "iter_count:441    the loss:-0.000338\n",
      "iter_count:442    the loss:-0.000020\n",
      "iter_count:443    the loss:-0.000282\n",
      "iter_count:444    the loss:-0.000091\n",
      "iter_count:445    the loss:-0.000293\n",
      "iter_count:446    the loss:-0.000013\n",
      "iter_count:447    the loss:-0.000298\n",
      "iter_count:448    the loss:-0.000048\n",
      "iter_count:449    the loss:-0.000281\n",
      "iter_count:450    the loss:-0.000253\n",
      "iter_count:451    the loss:-0.000146\n",
      "iter_count:452    the loss:0.000060\n",
      "iter_count:453    the loss:0.000183\n",
      "iter_count:454    the loss:-0.000113\n",
      "iter_count:455    the loss:0.000074\n",
      "iter_count:456    the loss:0.000108\n",
      "iter_count:457    the loss:0.000247\n",
      "iter_count:458    the loss:0.000039\n",
      "iter_count:459    the loss:0.000183\n",
      "iter_count:460    the loss:-0.000256\n",
      "iter_count:461    the loss:-0.000055\n",
      "iter_count:462    the loss:0.000014\n",
      "iter_count:463    the loss:0.000026\n",
      "iter_count:464    the loss:-0.000163\n",
      "iter_count:465    the loss:-0.000026\n",
      "iter_count:466    the loss:-0.000168\n",
      "iter_count:467    the loss:-0.000162\n",
      "iter_count:468    the loss:-0.000021\n",
      "iter_count:469    the loss:0.000005\n",
      "[3.00000128 3.99999933]\n",
      "iter_count:1    the loss:-36.347755\n",
      "iter_count:2    the loss:-9.607247\n",
      "iter_count:3    the loss:-2.717451\n",
      "iter_count:4    the loss:-1.258607\n",
      "iter_count:5    the loss:0.802615\n",
      "iter_count:6    the loss:-1.234025\n",
      "iter_count:7    the loss:-1.395512\n",
      "iter_count:8    the loss:-0.912361\n",
      "iter_count:9    the loss:-1.909054\n",
      "iter_count:10    the loss:-2.514769\n",
      "iter_count:11    the loss:-0.872420\n",
      "iter_count:12    the loss:-2.015253\n",
      "iter_count:13    the loss:-1.432660\n",
      "iter_count:14    the loss:-0.729105\n",
      "iter_count:15    the loss:-0.718965\n",
      "iter_count:16    the loss:-0.534044\n",
      "iter_count:17    the loss:0.143065\n",
      "iter_count:18    the loss:0.831489\n",
      "iter_count:19    the loss:-0.587422\n",
      "iter_count:20    the loss:-1.253021\n",
      "iter_count:21    the loss:-1.183599\n",
      "iter_count:22    the loss:-1.089770\n",
      "iter_count:23    the loss:-0.887345\n",
      "iter_count:24    the loss:-1.122638\n",
      "iter_count:25    the loss:-1.078011\n",
      "iter_count:26    the loss:-0.864348\n",
      "iter_count:27    the loss:-1.489746\n",
      "iter_count:28    the loss:-1.071703\n",
      "iter_count:29    the loss:-0.476059\n",
      "iter_count:30    the loss:-2.005197\n",
      "iter_count:31    the loss:-0.644313\n",
      "iter_count:32    the loss:0.105845\n",
      "iter_count:33    the loss:-0.354541\n",
      "iter_count:34    the loss:-1.435954\n",
      "iter_count:35    the loss:-0.755552\n",
      "iter_count:36    the loss:0.106707\n",
      "iter_count:37    the loss:0.623212\n",
      "iter_count:38    the loss:-0.872465\n",
      "iter_count:39    the loss:-0.863759\n",
      "iter_count:40    the loss:-0.707030\n",
      "iter_count:41    the loss:0.245210\n",
      "iter_count:42    the loss:-0.233768\n",
      "iter_count:43    the loss:-1.203887\n",
      "iter_count:44    the loss:-1.391227\n",
      "iter_count:45    the loss:-0.424860\n",
      "iter_count:46    the loss:0.131126\n",
      "iter_count:47    the loss:0.422821\n",
      "iter_count:48    the loss:-1.565876\n",
      "iter_count:49    the loss:-0.290829\n",
      "iter_count:50    the loss:0.393828\n",
      "iter_count:51    the loss:-0.575659\n",
      "iter_count:52    the loss:-1.283069\n",
      "iter_count:53    the loss:-1.166310\n",
      "iter_count:54    the loss:-0.722831\n",
      "iter_count:55    the loss:-0.608458\n",
      "iter_count:56    the loss:-0.323870\n",
      "iter_count:57    the loss:-1.079860\n",
      "iter_count:58    the loss:-0.043753\n",
      "iter_count:59    the loss:-0.208646\n",
      "iter_count:60    the loss:-0.162579\n",
      "iter_count:61    the loss:-0.710104\n",
      "iter_count:62    the loss:-0.701929\n",
      "iter_count:63    the loss:-1.035892\n",
      "iter_count:64    the loss:-0.199702\n",
      "iter_count:65    the loss:-0.680218\n",
      "iter_count:66    the loss:-0.937297\n",
      "iter_count:67    the loss:-0.407604\n",
      "iter_count:68    the loss:0.345332\n",
      "iter_count:69    the loss:0.226111\n",
      "iter_count:70    the loss:0.288958\n",
      "iter_count:71    the loss:-0.831201\n",
      "iter_count:72    the loss:-0.175583\n",
      "iter_count:73    the loss:0.009803\n",
      "iter_count:74    the loss:-0.030137\n",
      "iter_count:75    the loss:-0.438322\n",
      "iter_count:76    the loss:-0.898482\n",
      "iter_count:77    the loss:-0.463247\n",
      "iter_count:78    the loss:-0.085353\n",
      "iter_count:79    the loss:-0.412308\n",
      "iter_count:80    the loss:-0.214731\n",
      "iter_count:81    the loss:-0.703855\n",
      "iter_count:82    the loss:-0.737426\n",
      "iter_count:83    the loss:-0.426936\n",
      "iter_count:84    the loss:-0.830666\n",
      "iter_count:85    the loss:-0.387097\n",
      "iter_count:86    the loss:-0.193417\n",
      "iter_count:87    the loss:0.118563\n",
      "iter_count:88    the loss:0.102249\n",
      "iter_count:89    the loss:-0.568877\n",
      "iter_count:90    the loss:-0.482631\n",
      "iter_count:91    the loss:0.099482\n",
      "iter_count:92    the loss:-0.233781\n",
      "iter_count:93    the loss:-0.644691\n",
      "iter_count:94    the loss:-0.198903\n",
      "iter_count:95    the loss:-0.164696\n",
      "iter_count:96    the loss:-0.065283\n",
      "iter_count:97    the loss:-0.087868\n",
      "iter_count:98    the loss:-0.239923\n",
      "iter_count:99    the loss:-0.509896\n",
      "iter_count:100    the loss:-0.405077\n",
      "iter_count:101    the loss:-0.518100\n",
      "iter_count:102    the loss:-0.509101\n",
      "iter_count:103    the loss:-0.509186\n",
      "iter_count:104    the loss:-0.323500\n",
      "iter_count:105    the loss:-0.578970\n",
      "iter_count:106    the loss:-0.354397\n",
      "iter_count:107    the loss:-0.367394\n",
      "iter_count:108    the loss:-0.191452\n",
      "iter_count:109    the loss:-0.212028\n",
      "iter_count:110    the loss:-0.300015\n",
      "iter_count:111    the loss:-0.555433\n",
      "iter_count:112    the loss:-0.374745\n",
      "iter_count:113    the loss:-0.374338\n",
      "iter_count:114    the loss:-0.295642\n",
      "iter_count:115    the loss:-0.185399\n",
      "iter_count:116    the loss:-0.354693\n",
      "iter_count:117    the loss:-0.115016\n",
      "iter_count:118    the loss:-0.011138\n",
      "iter_count:119    the loss:-0.269790\n",
      "iter_count:120    the loss:-0.558454\n",
      "iter_count:121    the loss:-0.252421\n",
      "iter_count:122    the loss:-0.353388\n",
      "iter_count:123    the loss:-0.307004\n",
      "iter_count:124    the loss:-0.605347\n",
      "iter_count:125    the loss:-0.150179\n",
      "iter_count:126    the loss:-0.444807\n",
      "iter_count:127    the loss:-0.355466\n",
      "iter_count:128    the loss:-0.413488\n",
      "iter_count:129    the loss:-0.083900\n",
      "iter_count:130    the loss:-0.201742\n",
      "iter_count:131    the loss:-0.288648\n",
      "iter_count:132    the loss:-0.027744\n",
      "iter_count:133    the loss:-0.308317\n",
      "iter_count:134    the loss:-0.184129\n",
      "iter_count:135    the loss:-0.076957\n",
      "iter_count:136    the loss:-0.163257\n",
      "iter_count:137    the loss:-0.092335\n",
      "iter_count:138    the loss:0.006145\n",
      "iter_count:139    the loss:-0.198254\n",
      "iter_count:140    the loss:-0.179755\n",
      "iter_count:141    the loss:-0.414678\n",
      "iter_count:142    the loss:-0.069028\n",
      "iter_count:143    the loss:-0.114783\n",
      "iter_count:144    the loss:-0.357345\n",
      "iter_count:145    the loss:-0.072427\n",
      "iter_count:146    the loss:0.036115\n",
      "iter_count:147    the loss:0.047708\n",
      "iter_count:148    the loss:-0.336342\n",
      "iter_count:149    the loss:-0.143373\n",
      "iter_count:150    the loss:-0.292923\n",
      "iter_count:151    the loss:-0.344893\n",
      "iter_count:152    the loss:-0.143565\n",
      "iter_count:153    the loss:-0.188905\n",
      "iter_count:154    the loss:-0.058845\n",
      "iter_count:155    the loss:-0.161038\n",
      "iter_count:156    the loss:-0.340661\n",
      "iter_count:157    the loss:-0.102477\n",
      "iter_count:158    the loss:0.042031\n",
      "iter_count:159    the loss:-0.225453\n",
      "iter_count:160    the loss:-0.134017\n",
      "iter_count:161    the loss:-0.033437\n",
      "iter_count:162    the loss:-0.132594\n",
      "iter_count:163    the loss:-0.202513\n",
      "iter_count:164    the loss:-0.028525\n",
      "iter_count:165    the loss:-0.233856\n",
      "iter_count:166    the loss:-0.188510\n",
      "iter_count:167    the loss:-0.099936\n",
      "iter_count:168    the loss:-0.293330\n",
      "iter_count:169    the loss:-0.206880\n",
      "iter_count:170    the loss:-0.169474\n",
      "iter_count:171    the loss:-0.262224\n",
      "iter_count:172    the loss:-0.177660\n",
      "iter_count:173    the loss:-0.208088\n",
      "iter_count:174    the loss:-0.243112\n",
      "iter_count:175    the loss:-0.160813\n",
      "iter_count:176    the loss:-0.131303\n",
      "iter_count:177    the loss:-0.110011\n",
      "iter_count:178    the loss:-0.085610\n",
      "iter_count:179    the loss:-0.108665\n",
      "iter_count:180    the loss:-0.254663\n",
      "iter_count:181    the loss:-0.181756\n",
      "iter_count:182    the loss:-0.009201\n",
      "iter_count:183    the loss:0.014724\n",
      "iter_count:184    the loss:-0.069339\n",
      "iter_count:185    the loss:-0.035510\n",
      "iter_count:186    the loss:0.016172\n",
      "iter_count:187    the loss:-0.017734\n",
      "iter_count:188    the loss:-0.033548\n",
      "iter_count:189    the loss:-0.009869\n",
      "iter_count:190    the loss:-0.051334\n",
      "iter_count:191    the loss:-0.114323\n",
      "iter_count:192    the loss:-0.063052\n",
      "iter_count:193    the loss:-0.097105\n",
      "iter_count:194    the loss:-0.109902\n",
      "iter_count:195    the loss:-0.009143\n",
      "iter_count:196    the loss:-0.174975\n",
      "iter_count:197    the loss:-0.116711\n",
      "iter_count:198    the loss:-0.159877\n",
      "iter_count:199    the loss:-0.071186\n",
      "iter_count:200    the loss:-0.076143\n",
      "iter_count:201    the loss:0.046182\n",
      "iter_count:202    the loss:0.014593\n",
      "iter_count:203    the loss:-0.214591\n",
      "iter_count:204    the loss:-0.066956\n",
      "iter_count:205    the loss:-0.024634\n",
      "iter_count:206    the loss:0.012152\n",
      "iter_count:207    the loss:-0.004016\n",
      "iter_count:208    the loss:-0.044243\n",
      "iter_count:209    the loss:-0.145614\n",
      "iter_count:210    the loss:-0.141187\n",
      "iter_count:211    the loss:-0.034125\n",
      "iter_count:212    the loss:-0.073494\n",
      "iter_count:213    the loss:-0.105584\n",
      "iter_count:214    the loss:-0.078293\n",
      "iter_count:215    the loss:0.018573\n",
      "iter_count:216    the loss:0.004874\n",
      "iter_count:217    the loss:-0.105276\n",
      "iter_count:218    the loss:0.020008\n",
      "iter_count:219    the loss:-0.066524\n",
      "iter_count:220    the loss:-0.042380\n",
      "iter_count:221    the loss:-0.031204\n",
      "iter_count:222    the loss:-0.111336\n",
      "iter_count:223    the loss:-0.096381\n",
      "iter_count:224    the loss:-0.052782\n",
      "iter_count:225    the loss:-0.070438\n",
      "iter_count:226    the loss:-0.116603\n",
      "iter_count:227    the loss:-0.053677\n",
      "iter_count:228    the loss:-0.036118\n",
      "iter_count:229    the loss:0.007672\n",
      "iter_count:230    the loss:-0.024901\n",
      "iter_count:231    the loss:-0.029250\n",
      "iter_count:232    the loss:-0.006068\n",
      "iter_count:233    the loss:-0.088860\n",
      "iter_count:234    the loss:-0.044908\n",
      "iter_count:235    the loss:-0.074054\n",
      "iter_count:236    the loss:-0.020484\n",
      "iter_count:237    the loss:-0.036982\n",
      "iter_count:238    the loss:-0.030571\n",
      "iter_count:239    the loss:-0.028736\n",
      "iter_count:240    the loss:0.023167\n",
      "iter_count:241    the loss:-0.078755\n",
      "iter_count:242    the loss:-0.043450\n",
      "iter_count:243    the loss:-0.031422\n",
      "iter_count:244    the loss:-0.051224\n",
      "iter_count:245    the loss:-0.081163\n",
      "iter_count:246    the loss:-0.076940\n",
      "iter_count:247    the loss:-0.057650\n",
      "iter_count:248    the loss:0.007534\n",
      "iter_count:249    the loss:-0.024962\n",
      "iter_count:250    the loss:-0.075827\n",
      "iter_count:251    the loss:-0.018191\n",
      "iter_count:252    the loss:-0.054539\n",
      "iter_count:253    the loss:-0.008896\n",
      "iter_count:254    the loss:-0.029437\n",
      "iter_count:255    the loss:-0.037873\n",
      "iter_count:256    the loss:0.018097\n",
      "iter_count:257    the loss:0.024087\n",
      "iter_count:258    the loss:-0.076712\n",
      "iter_count:259    the loss:-0.030048\n",
      "iter_count:260    the loss:0.025542\n",
      "iter_count:261    the loss:-0.048763\n",
      "iter_count:262    the loss:-0.040614\n",
      "iter_count:263    the loss:-0.076613\n",
      "iter_count:264    the loss:-0.051076\n",
      "iter_count:265    the loss:-0.006929\n",
      "iter_count:266    the loss:-0.036425\n",
      "iter_count:267    the loss:-0.050188\n",
      "iter_count:268    the loss:-0.038232\n",
      "iter_count:269    the loss:-0.061185\n",
      "iter_count:270    the loss:-0.032272\n",
      "iter_count:271    the loss:-0.038208\n",
      "iter_count:272    the loss:-0.055808\n",
      "iter_count:273    the loss:-0.030009\n",
      "iter_count:274    the loss:-0.030371\n",
      "iter_count:275    the loss:-0.001088\n",
      "iter_count:276    the loss:-0.007604\n",
      "iter_count:277    the loss:-0.012256\n",
      "iter_count:278    the loss:0.000638\n",
      "iter_count:279    the loss:-0.014703\n",
      "iter_count:280    the loss:-0.012837\n",
      "iter_count:281    the loss:-0.018412\n",
      "iter_count:282    the loss:-0.027248\n",
      "iter_count:283    the loss:0.013384\n",
      "iter_count:284    the loss:-0.023170\n",
      "iter_count:285    the loss:-0.022165\n",
      "iter_count:286    the loss:-0.029561\n",
      "iter_count:287    the loss:-0.029541\n",
      "iter_count:288    the loss:-0.001380\n",
      "iter_count:289    the loss:-0.020485\n",
      "iter_count:290    the loss:-0.004785\n",
      "iter_count:291    the loss:-0.020675\n",
      "iter_count:292    the loss:-0.001256\n",
      "iter_count:293    the loss:-0.031900\n",
      "iter_count:294    the loss:-0.004985\n",
      "iter_count:295    the loss:0.010583\n",
      "iter_count:296    the loss:-0.022673\n",
      "iter_count:297    the loss:-0.025687\n",
      "iter_count:298    the loss:-0.036793\n",
      "iter_count:299    the loss:-0.026829\n",
      "iter_count:300    the loss:-0.006567\n",
      "iter_count:301    the loss:-0.025848\n",
      "iter_count:302    the loss:-0.036945\n",
      "iter_count:303    the loss:-0.014800\n",
      "iter_count:304    the loss:0.001290\n",
      "iter_count:305    the loss:-0.016011\n",
      "iter_count:306    the loss:-0.041145\n",
      "iter_count:307    the loss:-0.035408\n",
      "iter_count:308    the loss:-0.007355\n",
      "iter_count:309    the loss:-0.029079\n",
      "iter_count:310    the loss:-0.018049\n",
      "iter_count:311    the loss:-0.007264\n",
      "iter_count:312    the loss:-0.016496\n",
      "iter_count:313    the loss:-0.020413\n",
      "iter_count:314    the loss:-0.012984\n",
      "iter_count:315    the loss:-0.003374\n",
      "iter_count:316    the loss:-0.006438\n",
      "iter_count:317    the loss:-0.019535\n",
      "iter_count:318    the loss:-0.006260\n",
      "iter_count:319    the loss:-0.011132\n",
      "iter_count:320    the loss:-0.019404\n",
      "iter_count:321    the loss:-0.014674\n",
      "iter_count:322    the loss:-0.009890\n",
      "iter_count:323    the loss:-0.015518\n",
      "iter_count:324    the loss:-0.012022\n",
      "iter_count:325    the loss:-0.023251\n",
      "iter_count:326    the loss:-0.013162\n",
      "iter_count:327    the loss:-0.008669\n",
      "iter_count:328    the loss:-0.020048\n",
      "iter_count:329    the loss:-0.012719\n",
      "iter_count:330    the loss:0.004696\n",
      "iter_count:331    the loss:-0.026159\n",
      "iter_count:332    the loss:-0.025341\n",
      "iter_count:333    the loss:0.000115\n",
      "iter_count:334    the loss:0.000175\n",
      "iter_count:335    the loss:-0.007827\n",
      "iter_count:336    the loss:-0.000359\n",
      "iter_count:337    the loss:-0.006544\n",
      "iter_count:338    the loss:-0.016053\n",
      "iter_count:339    the loss:-0.022912\n",
      "iter_count:340    the loss:-0.016755\n",
      "iter_count:341    the loss:-0.012476\n",
      "iter_count:342    the loss:0.000941\n",
      "iter_count:343    the loss:0.002017\n",
      "iter_count:344    the loss:-0.003494\n",
      "iter_count:345    the loss:0.003088\n",
      "iter_count:346    the loss:-0.020958\n",
      "iter_count:347    the loss:-0.007640\n",
      "iter_count:348    the loss:-0.017991\n",
      "iter_count:349    the loss:-0.012999\n",
      "iter_count:350    the loss:-0.014502\n",
      "iter_count:351    the loss:-0.013650\n",
      "iter_count:352    the loss:-0.003403\n",
      "iter_count:353    the loss:-0.010407\n",
      "iter_count:354    the loss:-0.009510\n",
      "iter_count:355    the loss:-0.015162\n",
      "iter_count:356    the loss:-0.012450\n",
      "iter_count:357    the loss:-0.017029\n",
      "iter_count:358    the loss:-0.009458\n",
      "iter_count:359    the loss:-0.002841\n",
      "iter_count:360    the loss:-0.004176\n",
      "iter_count:361    the loss:-0.003308\n",
      "iter_count:362    the loss:-0.007537\n",
      "iter_count:363    the loss:-0.013309\n",
      "iter_count:364    the loss:-0.000244\n",
      "iter_count:365    the loss:-0.011099\n",
      "iter_count:366    the loss:-0.006407\n",
      "iter_count:367    the loss:-0.014011\n",
      "iter_count:368    the loss:-0.007427\n",
      "iter_count:369    the loss:-0.002951\n",
      "iter_count:370    the loss:-0.004276\n",
      "iter_count:371    the loss:-0.005091\n",
      "iter_count:372    the loss:-0.005704\n",
      "iter_count:373    the loss:-0.009792\n",
      "iter_count:374    the loss:-0.004599\n",
      "iter_count:375    the loss:-0.002511\n",
      "iter_count:376    the loss:-0.004533\n",
      "iter_count:377    the loss:-0.010250\n",
      "iter_count:378    the loss:-0.001765\n",
      "iter_count:379    the loss:0.000941\n",
      "iter_count:380    the loss:-0.008039\n",
      "iter_count:381    the loss:-0.002976\n",
      "iter_count:382    the loss:-0.002424\n",
      "iter_count:383    the loss:-0.010610\n",
      "iter_count:384    the loss:-0.006098\n",
      "iter_count:385    the loss:-0.009405\n",
      "iter_count:386    the loss:-0.007892\n",
      "iter_count:387    the loss:-0.005235\n",
      "iter_count:388    the loss:-0.001182\n",
      "iter_count:389    the loss:-0.003209\n",
      "iter_count:390    the loss:-0.009243\n",
      "iter_count:391    the loss:0.000288\n",
      "iter_count:392    the loss:-0.011414\n",
      "iter_count:393    the loss:-0.008925\n",
      "iter_count:394    the loss:-0.010042\n",
      "iter_count:395    the loss:-0.004355\n",
      "iter_count:396    the loss:-0.002939\n",
      "iter_count:397    the loss:0.001437\n",
      "iter_count:398    the loss:-0.003614\n",
      "iter_count:399    the loss:-0.007070\n",
      "iter_count:400    the loss:-0.006378\n",
      "iter_count:401    the loss:-0.005779\n",
      "iter_count:402    the loss:0.001929\n",
      "iter_count:403    the loss:-0.005513\n",
      "iter_count:404    the loss:-0.001396\n",
      "iter_count:405    the loss:-0.000220\n",
      "iter_count:406    the loss:-0.007615\n",
      "iter_count:407    the loss:-0.003278\n",
      "iter_count:408    the loss:-0.006323\n",
      "iter_count:409    the loss:-0.002756\n",
      "iter_count:410    the loss:-0.008715\n",
      "iter_count:411    the loss:-0.008498\n",
      "iter_count:412    the loss:-0.004675\n",
      "iter_count:413    the loss:-0.001482\n",
      "iter_count:414    the loss:-0.001579\n",
      "iter_count:415    the loss:-0.002697\n",
      "iter_count:416    the loss:-0.000672\n",
      "iter_count:417    the loss:-0.003419\n",
      "iter_count:418    the loss:-0.003587\n",
      "iter_count:419    the loss:-0.007690\n",
      "iter_count:420    the loss:-0.007376\n",
      "iter_count:421    the loss:-0.003423\n",
      "iter_count:422    the loss:-0.001157\n",
      "iter_count:423    the loss:-0.000623\n",
      "iter_count:424    the loss:-0.006380\n",
      "iter_count:425    the loss:-0.004547\n",
      "iter_count:426    the loss:-0.006220\n",
      "iter_count:427    the loss:-0.003596\n",
      "iter_count:428    the loss:-0.006527\n",
      "iter_count:429    the loss:-0.003091\n",
      "iter_count:430    the loss:-0.001431\n",
      "iter_count:431    the loss:-0.003697\n",
      "iter_count:432    the loss:-0.002469\n",
      "iter_count:433    the loss:-0.002854\n",
      "iter_count:434    the loss:-0.002297\n",
      "iter_count:435    the loss:0.000055\n",
      "iter_count:436    the loss:-0.004419\n",
      "iter_count:437    the loss:-0.005001\n",
      "iter_count:438    the loss:-0.003659\n",
      "iter_count:439    the loss:0.000539\n",
      "iter_count:440    the loss:0.002813\n",
      "iter_count:441    the loss:-0.003216\n",
      "iter_count:442    the loss:-0.003895\n",
      "iter_count:443    the loss:-0.000142\n",
      "iter_count:444    the loss:-0.005494\n",
      "iter_count:445    the loss:-0.001540\n",
      "iter_count:446    the loss:-0.002379\n",
      "iter_count:447    the loss:-0.000709\n",
      "iter_count:448    the loss:-0.003387\n",
      "iter_count:449    the loss:-0.004235\n",
      "iter_count:450    the loss:-0.003600\n",
      "iter_count:451    the loss:-0.000684\n",
      "iter_count:452    the loss:-0.002101\n",
      "iter_count:453    the loss:-0.001086\n",
      "iter_count:454    the loss:-0.003076\n",
      "iter_count:455    the loss:-0.000392\n",
      "iter_count:456    the loss:0.000878\n",
      "iter_count:457    the loss:-0.002722\n",
      "iter_count:458    the loss:-0.000089\n",
      "iter_count:459    the loss:-0.001805\n",
      "iter_count:460    the loss:-0.002153\n",
      "iter_count:461    the loss:-0.001787\n",
      "iter_count:462    the loss:-0.003814\n",
      "iter_count:463    the loss:-0.003628\n",
      "iter_count:464    the loss:-0.001719\n",
      "iter_count:465    the loss:-0.000703\n",
      "iter_count:466    the loss:-0.001255\n",
      "iter_count:467    the loss:-0.001443\n",
      "iter_count:468    the loss:-0.000254\n",
      "iter_count:469    the loss:-0.002777\n",
      "iter_count:470    the loss:-0.001504\n",
      "iter_count:471    the loss:0.000170\n",
      "iter_count:472    the loss:-0.000987\n",
      "iter_count:473    the loss:0.000900\n",
      "iter_count:474    the loss:-0.001817\n",
      "iter_count:475    the loss:-0.000221\n",
      "iter_count:476    the loss:-0.001792\n",
      "iter_count:477    the loss:-0.001947\n",
      "iter_count:478    the loss:-0.002808\n",
      "iter_count:479    the loss:-0.001621\n",
      "iter_count:480    the loss:-0.000750\n",
      "iter_count:481    the loss:-0.000496\n",
      "iter_count:482    the loss:-0.003023\n",
      "iter_count:483    the loss:-0.003377\n",
      "iter_count:484    the loss:-0.002195\n",
      "iter_count:485    the loss:-0.002923\n",
      "iter_count:486    the loss:-0.001447\n",
      "iter_count:487    the loss:-0.000125\n",
      "iter_count:488    the loss:-0.001928\n",
      "iter_count:489    the loss:-0.001729\n",
      "iter_count:490    the loss:-0.000431\n",
      "iter_count:491    the loss:-0.001888\n",
      "iter_count:492    the loss:-0.001315\n",
      "iter_count:493    the loss:-0.000856\n",
      "iter_count:494    the loss:-0.002130\n",
      "iter_count:495    the loss:-0.002250\n",
      "iter_count:496    the loss:0.000077\n",
      "iter_count:497    the loss:-0.001960\n",
      "iter_count:498    the loss:-0.001976\n",
      "iter_count:499    the loss:-0.002060\n",
      "iter_count:500    the loss:-0.001062\n",
      "iter_count:501    the loss:0.000086\n",
      "iter_count:502    the loss:-0.001421\n",
      "iter_count:503    the loss:-0.001626\n",
      "iter_count:504    the loss:-0.000662\n",
      "iter_count:505    the loss:0.000440\n",
      "iter_count:506    the loss:0.000504\n",
      "iter_count:507    the loss:-0.001372\n",
      "iter_count:508    the loss:-0.001796\n",
      "iter_count:509    the loss:-0.001140\n",
      "iter_count:510    the loss:-0.001114\n",
      "iter_count:511    the loss:-0.001237\n",
      "iter_count:512    the loss:-0.001537\n",
      "iter_count:513    the loss:-0.000431\n",
      "iter_count:514    the loss:-0.001699\n",
      "iter_count:515    the loss:-0.000800\n",
      "iter_count:516    the loss:-0.000858\n",
      "iter_count:517    the loss:-0.000997\n",
      "iter_count:518    the loss:-0.000582\n",
      "iter_count:519    the loss:-0.001386\n",
      "iter_count:520    the loss:-0.000358\n",
      "iter_count:521    the loss:-0.000211\n",
      "iter_count:522    the loss:-0.001270\n",
      "iter_count:523    the loss:-0.000826\n",
      "iter_count:524    the loss:-0.000848\n",
      "iter_count:525    the loss:-0.000223\n",
      "iter_count:526    the loss:-0.000602\n",
      "iter_count:527    the loss:-0.000738\n",
      "iter_count:528    the loss:-0.000294\n",
      "iter_count:529    the loss:0.000470\n",
      "iter_count:530    the loss:-0.001028\n",
      "iter_count:531    the loss:-0.000478\n",
      "iter_count:532    the loss:-0.000233\n",
      "iter_count:533    the loss:0.000187\n",
      "iter_count:534    the loss:-0.000168\n",
      "iter_count:535    the loss:-0.001198\n",
      "iter_count:536    the loss:-0.001058\n",
      "iter_count:537    the loss:-0.000144\n",
      "iter_count:538    the loss:-0.000823\n",
      "iter_count:539    the loss:-0.000891\n",
      "iter_count:540    the loss:-0.000223\n",
      "iter_count:541    the loss:-0.000870\n",
      "iter_count:542    the loss:-0.000635\n",
      "iter_count:543    the loss:-0.001107\n",
      "iter_count:544    the loss:-0.000621\n",
      "iter_count:545    the loss:-0.000755\n",
      "iter_count:546    the loss:-0.000626\n",
      "iter_count:547    the loss:-0.000076\n",
      "iter_count:548    the loss:-0.000661\n",
      "iter_count:549    the loss:-0.000282\n",
      "iter_count:550    the loss:-0.000551\n",
      "iter_count:551    the loss:0.000602\n",
      "iter_count:552    the loss:-0.000600\n",
      "iter_count:553    the loss:-0.000789\n",
      "iter_count:554    the loss:-0.000416\n",
      "iter_count:555    the loss:-0.000410\n",
      "iter_count:556    the loss:-0.000194\n",
      "iter_count:557    the loss:-0.000182\n",
      "iter_count:558    the loss:-0.000653\n",
      "iter_count:559    the loss:-0.000409\n",
      "iter_count:560    the loss:-0.000839\n",
      "iter_count:561    the loss:-0.000645\n",
      "iter_count:562    the loss:-0.000834\n",
      "iter_count:563    the loss:-0.000475\n",
      "iter_count:564    the loss:-0.000574\n",
      "iter_count:565    the loss:-0.000715\n",
      "iter_count:566    the loss:-0.000520\n",
      "iter_count:567    the loss:-0.000457\n",
      "iter_count:568    the loss:-0.000228\n",
      "iter_count:569    the loss:-0.000439\n",
      "iter_count:570    the loss:-0.000640\n",
      "iter_count:571    the loss:-0.000157\n",
      "iter_count:572    the loss:-0.000413\n",
      "iter_count:573    the loss:0.000051\n",
      "iter_count:574    the loss:-0.000305\n",
      "iter_count:575    the loss:0.000273\n",
      "iter_count:576    the loss:0.000121\n",
      "iter_count:577    the loss:-0.000216\n",
      "iter_count:578    the loss:0.000008\n",
      "[3.00010708 3.9999434 ]\n"
     ]
    }
   ],
   "source": [
    "x, y = get_data()\n",
    "time1 = time.time()\n",
    "print(bgd(x, y))\n",
    "time2 = time.time()\n",
    "print(sgd(x, y))\n",
    "time3 = time.time()\n",
    "print(msgd(x, y, 10))\n",
    "time4 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bgd  0.0329129695892334\n",
      "sgd  0.025929689407348633\n",
      "msgd  0.07679390907287598\n"
     ]
    }
   ],
   "source": [
    "print('bgd ',time2-time1)\n",
    "print('sgd ',time3-time2)\n",
    "print('msgd ',time4-time3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch实现梯度下降法\n",
    "首先随机生成N个数，再通过一个线性函数生成目标数据target，然后将这N个数据通过我们自定义的神经网络得到输出output，通过不断迭代更新w和b，最小化target和output的差值，以此来实现：输入最初产生的随机数据N，通过网络，得到近似线性函数生成的目标数据target。\n",
    "https://blog.csdn.net/m0_37673307/article/details/82315176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#from torch.autograd import Variable\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as f\n",
    "#import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义网络\n",
    "NUM = 100   #输入 输出个数（输入层神经元个数） 线性回归\n",
    "hider_num = 300  #隐藏层神经元个数\n",
    "class Net(torch.nn.Module): #继承父类，\n",
    "    def __init__(self):#构造函数\n",
    "        super(Net,self).__init__() # 必须在构造函数中执行父类的构造函数\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(NUM,hider_num) #把网络中具有 可学习参数的层 放在 构造函数中\n",
    "        self.fc2 = torch.nn.Linear(hider_num,NUM) #实现线性回归，输入和输出个数一致\n",
    "    def forward(self,x): #实现 nn.Module的forward方法，前向传播的计算过程\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1均匀分布torch.rand(*sizes, out=None) → Tensor\n",
    "返回一个张量，包含了从区间[0, 1)的均匀分布中抽取的一组随机数。张量的形状由参数sizes定义。\n",
    "2标准正态分布torch.randn(*sizes, out=None) → Tensor\n",
    "返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数sizes定义。\n",
    "3 离散正态分布torch.normal(means, std, out=None) → → Tensor\n",
    "4线性间距向量torch.linspace(start, end, steps=100, out=None) → Tensor\n",
    "返回一个1维张量，包含在区间start和end上均匀间隔的step个点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "#生成数据 x-target\n",
    "X = torch.randn(NUM)#标准正态分布，维度为一维NUM长度向量？，Tensor类型数据,,requires_grad=True\n",
    "#要想使得Tensor使用autograd功能，只需要设置tensor.requries_grad=True\n",
    "print(input.size())\n",
    "target = torch.Tensor(0.5 *X+ 0.3) #用0.5 × x + 0.3 函生成目标数据\n",
    "print(target.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=100, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#网络更新\n",
    "net = Net()   #初始化网络\n",
    "print(net)\n",
    " \n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.01)  #随机梯度下降优化器\n",
    "loss_list =[]                                #保存loss，便于画图\n",
    "epochs = 500                                    #迭代次数\n",
    " \n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()                    #参数梯度清零，因为会累加\n",
    "    out = net(X)                         #通过一次forward 网络的输出\n",
    "    loss = torch.nn.MSELoss()(out,target)           #计算输出与target数据的均方差\n",
    "    loss_list.append(loss)                    #保存loss\n",
    "    loss.backward()                           #loss反向传播\n",
    "    optimizer.step()                          #更新参数w，b,所有样本过一次，计算总loss值，参数更新一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 的层级--------->\n",
      "<MseLossBackward object at 0x000001DC1F117048>\n",
      "<AddBackward0 object at 0x000001DC1F0A5128>\n",
      "<SqueezeBackward3 object at 0x000001DC1F117208>\n",
      "<MmBackward object at 0x000001DC1F0A5128>\n",
      "<UnsqueezeBackward0 object at 0x000001DC1F117208>\n",
      "<ReluBackward0 object at 0x000001DC1F0A5128>\n",
      "<AddBackward0 object at 0x000001DC1F117048>\n",
      "<SqueezeBackward3 object at 0x000001DC1F0A5128>\n",
      "<MmBackward object at 0x000001DC1F117048>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nVariable主要包含三个属性。 - data：保存Variable所包含的Tensor \\n                        - grad：保存data对应的梯度，grad也是个Variable，而不是Tensor，它和data的形状一样。 \\n                        - grad_fn：指向一个Function对象，这个Function用来反向传播计算输入的梯度，具体细节会在下一章讲解。\\n从0.4起, Variable 正式合并入Tensor, Variable 本来实现的自动微分功能，Tensor就能支持。\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('loss 的层级--------->')\n",
    "last_fn = loss.grad_fn    #y.grad_fn # pytorch 会自动调用autograd的所有记录操作\n",
    "while last_fn:\n",
    "    print(last_fn)\n",
    "    last_fn = last_fn.next_functions[0][0]  #不断找上一层function\n",
    "'''\n",
    "Variable主要包含三个属性。 - data：保存Variable所包含的Tensor \n",
    "                        - grad：保存data对应的梯度，grad也是个Variable，而不是Tensor，它和data的形状一样。 \n",
    "                        - grad_fn：指向一个Function对象，这个Function用来反向传播计算输入的梯度，具体细节会在下一章讲解。\n",
    "从0.4起, Variable 正式合并入Tensor, Variable 本来实现的自动微分功能，Tensor就能支持。\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " #画loss图\n",
    "plt.figure(1)\n",
    "plt.plot(range(1,epochs+1),loss_list,'o-',ms=3,lw=1,color='black')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "这里 隐藏层神经元个数 和 迭代次数 都对结果有关：上述是step = 500，hide_num = 300  的结果，不同的组合，loss下降的趋势，最终效果都会不同\n",
    "1）step = 500，hide_num = 30  \n",
    "2）step = 50，hide_num = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSGD"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DataLoader那有点问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xloader = d.DataLoader(X, \n",
    "                    batch_size=4,\n",
    "                    shuffle=True, \n",
    "                    num_workers=2)\n",
    "testloader = d.DataLoader(target,\n",
    "                    batch_size=4, \n",
    "                    shuffle=False,\n",
    "                    num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-5d300b7d807f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#??? i从0开始的意思？\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# 输入数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;31m#一次迭代 batch_size为4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;31m# 梯度清零\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "torch.set_num_threads(8)???\n",
    "不设置，cpu51%，时间15ms;2的时候，cpu17%,时间15ms变为25ms;4的时候，cpu34%，时间17ms;8的时候，cpu67%，\n",
    "'''\n",
    "criterion = torch.nn.MSELoss() # \n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # momentum=0.9？？？？\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.001)  #随机梯度下降优化器，momentum 动量 可选参数\n",
    "\n",
    "torch.set_num_threads(4) #num_threads 为8，线程数？\n",
    "for epoch in range(epochs):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0): #??? i从0开始的意思？\n",
    "        # 输入数据\n",
    "        inputs, labels = data #一次迭代 batch_size为4\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()    \n",
    "        # forward + backward \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward()       \n",
    "        # 每一个batch_size 更新一次参数 \n",
    "        optimizer.step()       \n",
    "        # 打印log信息\n",
    "        # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]\n",
    "        running_loss += loss.item()\n",
    "        if i % 25 == 24: # 每25batch打印一下训练状态\n",
    "            print('[%d, %5d] loss: %.3f' \\\n",
    "                  % (epoch+1, i+1, running_loss / 25)) #每2000个样本，计算平均loss值，打印\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch实现一个简单的神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # 转为Tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化，具体含义是？？\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "trainset = tv.datasets.CIFAR10(\n",
    "                    root='/home/cy/tmp/data/', \n",
    "                    train=True, \n",
    "                    download=True,\n",
    "                    transform=transform)  \n",
    "testset = tv.datasets.CIFAR10(\n",
    "                    '/home/cy/tmp/data/',\n",
    "                    train=False, \n",
    "                    download=True, \n",
    "                    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = d.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=4,\n",
    "                    shuffle=True, \n",
    "                    num_workers=2)\n",
    "testloader = d.DataLoader(\n",
    "                    testset,\n",
    "                    batch_size=4, \n",
    "                    shuffle=False,\n",
    "                    num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  \n",
    "        self.fc1   = nn.Linear(16*5*5, 120)  \n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) \n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) \n",
    "        x = x.view(x.size()[0], -1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数和优化器(loss和optimizer)\n",
    "from torch import optim\n",
    "criterion = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.403\n",
      "[1,  4000] loss: 1.400\n",
      "[1,  6000] loss: 1.372\n",
      "[1,  8000] loss: 1.335\n",
      "[1, 10000] loss: 1.303\n",
      "[1, 12000] loss: 1.316\n",
      "[2,  2000] loss: 1.243\n",
      "[2,  4000] loss: 1.228\n",
      "[2,  6000] loss: 1.242\n",
      "[2,  8000] loss: 1.226\n",
      "[2, 10000] loss: 1.210\n",
      "[2, 12000] loss: 1.201\n",
      "[3,  2000] loss: 1.113\n",
      "[3,  4000] loss: 1.149\n",
      "[3,  6000] loss: 1.137\n",
      "[3,  8000] loss: 1.119\n",
      "[3, 10000] loss: 1.137\n",
      "[3, 12000] loss: 1.128\n",
      "[4,  2000] loss: 1.049\n",
      "[4,  4000] loss: 1.067\n",
      "[4,  6000] loss: 1.087\n",
      "[4,  8000] loss: 1.067\n",
      "[4, 10000] loss: 1.058\n",
      "[4, 12000] loss: 1.062\n",
      "[5,  2000] loss: 0.993\n",
      "[5,  4000] loss: 1.010\n",
      "[5,  6000] loss: 1.020\n",
      "[5,  8000] loss: 1.000\n",
      "[5, 10000] loss: 1.008\n",
      "[5, 12000] loss: 1.034\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "torch.set_num_threads(4) #num_threads 为8，线程数？\n",
    "for epoch in range(5):  #共训练2个epoch\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0): #??? i从0开始的意思？\n",
    "        \n",
    "        # 输入数据\n",
    "        inputs, labels = data #一次迭代 batch_size为4\n",
    "        \n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward()   \n",
    "        \n",
    "        # 更新参数 \n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印log信息\n",
    "        # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999: # 每2000个batch打印一下训练状态\n",
    "            print('[%d, %5d] loss: %.3f' \\\n",
    "                  % (epoch+1, i+1, running_loss / 2000)) #每2000个样本，计算平均loss值，打印\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000张测试集中的准确率为: 61 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0 # 预测正确的图片数\n",
    "total = 0 # 总共的图片数\n",
    "\n",
    "# 由于测试的时候不需要求导，可以暂时关闭autograd，提高速度，节约内存\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1) #??\n",
    "        total += labels.size(0) #??\n",
    "        correct += (predicted == labels).sum() #??\n",
    "\n",
    "print('10000张测试集中的准确率为: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
